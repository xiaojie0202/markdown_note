# 网络爬虫
    顾名思义爬取网络资源的程序或脚本， 爬虫会安装一定的规则取爬去需要的信息
## 爬虫模块 request
    Requests 是使用 Apache2 Licensed 许可证的 基于Python开发的HTTP 库，其在Python内置模块的基础上进行了高度的封装，从而使得Pythoner进行网络请求时，变得美好了许多，使用Requests可以轻而易举的完成浏览器可有的任何操作。
### 安装
    pip3 install requests
### request模块详解
>- request 请求
```
respone = requests.get('url')  # 发送GET请求
respone = requests.post(url='url', data={},cookies={})  # 发送POST请求
respone = requests.put(url, data=None, **kwargs)
respone = requests.head(url, **kwargs)
respone = requests.delete(url, **kwargs)
respone = requests.patch(url, data=None, **kwargs)
respone = requests.options(url, **kwargs)

# 以上requests的方法都是基于requests.request()
 requests.request()
        - method：提交方式，post，get，delete， put， head， patch， options
        - url： 提交地址
        - params： 在url中传递参数，GET params = {k1:vi}
        - data: 在请求体里传递参数用于post请求 data = {k1:v1,k2:v2} or 'k1=v1&k2=v2'
        - json: 在请求体里传递参数，并在请求头中设置content-type： application/json
        - headers： 在请求头中添加数据
        - cookies: 网站cookies 在请求头中
        - files : 文件对象{'f1': open('s1.py', wb), 'f2': ('上传到服务器的文件名', oprn('s1.py', wb))}
        - auth : 认证使用 在 请求头中加入用户名密码
        - timeout ： 超时时间
        - allow_redirects: 是否允许重定向 bool
        - proxies: 代理
        = stream: 流,bool   用于下载文件
            ret = request.get('http://127.0.0.1:8888/test/', steam=True)
            for i in ret.iter.content():
                print(i)
         - cert: 证书 指定https SSL证书文件
         - verify = False https忽略证书存在

```
>- 获取请求结果
```
respone.text # 返回str类型
respone.content # 返回字节类型
response.encoding # 指定Response的编码
response.apparent_encoding # 返回改respones的编码
response.cookies,get_dict() # 获取cookie的字典形式
```
>- requests模块高阶函数 request.Session
```
request.Session: 自动管理Cookies信息
ret = request.Session()
ret.get('https://www.baidu.com')

```

## 解析response内容
    当爬虫爬去数据的时候，使用  bs4 来解析爬去的内容
## 安装
    pip3 install beautifuilsoup4
## bs4 模块详解
```
# 导入模块 fron bs4 import beautifuilsoup4

beautifuilsoup4: 用于解析html标签
    soup = BeautifulSoup(response.text, 'lxml')

    tag = soup.find(id='li') # 查找第一个id为li的标签
    tag = soup.find(name='div' id='item', attrs={'class': 'list'}, recursive=True, text='Lacie'， class_='item)
        - name： 标签名
        - id：标签的id属性
        - attrs：标签的属性
        - recursive：是否递归的去查找，false 只在儿子里找， true 在子子孙孙也找
        - text：标签的文本信息
        - class_:标签的class属性

    tag_list = soup.find_all('div) # fing_all 查找所有div 返回一个list 参数名同find

    # select 同css的选择器
    tag = soup.select('#li')

    tag.text # 返回标签的文本
    tag.string # 返回标签的内容
    tag.stripped_string # 递归循环获取内部所有标签文本
    tag.name # 返回标签名称
    tag.attrs # 返回标签的属性 字典类型

    tag.attre = {'id': 'hello'} # 重新赋值

    tag.clear() # 清空内部所有标签
    tag.decompose() # 清空所有标签包含自己
    tag.extract() # 同decompose 但是会返回删除的标签

    tag.encode() # 返回标签的字节类型
    tag.decode() # 返回标签的字符串类型
    tag.encode_contents() # 返回标签的内部内容字节类型
    tag.decode_contents() # 返回标签的内部内容字符串类型

    tag.get('id') # 获取标签的id属性
    tag.has_attr('id') # 判断当前标签是否有id这个属性
    tag.get_text() # 获取当前标签的文本

    tag.index(tag,find('div')) # 查找标签在某个标签中的索引位置
    tag.is_empty_element() 判断是否是自闭和标签： 如果 br,hr,input,img,meta,soacer,link,frame,base

    # 当前标签的关联标签，同jquery的链式调用
    tag.next
    tag.next_element
    tag.next_elements
    tag.next_sibling
    tag.next_siblings
    tag.previous
    tag.previous_element
    tag.previous_elements
    tag.previous_sibling
    tag.previous_siblings
    tag.parend
    tag.parents

    tag.children # 当前标签的所有儿子标签 返回一个列表
        children会返回换行
        # 获取所有标签，不要文本
        from bs4.element import Tag:
        for tag in tag.children:
            if type(tag) == Tag:
                print(tag)

    tag.descendants # 当前元素的所有子子孙孙标签

    # 当前标签的关联标签并支持筛选的 参数同find_all
    tag.fint_next()
    tag.fint_next_element()
    tag.fint_next_elements()
    tag.fint_next_sibling()
    tag.fint_next_siblings()
    tag.fint_previous()
    tag.fint_previous_element()
    tag.fint_previous_elements()
    tag.fint_previous_sibling()
    tag.fint_previous_siblings()
    tag.fint_parend()
    tag.fint_parents()


    from bs4.element import Tag:
    a = Tag(name='a', attrs={'herf':'http://www.baidu.com/'})
    tag = soup.find('body)

        tag..append(a)# 创建一个元素用于追加到当前元素后边
        tag.insert(2,a) # 在当前元素中插入一个元素
        tag.insert_after(a) # 在当前标签前插入
        tag.insert_before(a) # 在当前标签后插入
        tag.replace_with() # 当前标签替换为指定标签
        tag.wrap(a) # 包裹某个表情
        tag.unwrap() # 去除当前标签的包裹
```

## 实列
### 爬取汽车之家新闻，并下载新闻图片到本地
```
import requests
from bs4 import BeautifulSoup
from uuid import uuid4

# 发送get请求下载整个网站
response = requests.get(
    url='https://www.autohome.com.cn/news/')

# 指定编码为此网站的编码
response.encoding = response.apparent_encoding

# response.text 所有HTML标签的文本格式

# 下载的html文本转换成对象， 第二个参数指定html解析器为 lxml
soup = BeautifulSoup(response.text, 'lxml')

# 获取包含所有新闻列表的DIV
news_div = soup.find(id='auto-channel-lazyload-article')

# 查找news_div下的所有li标签  list类型
news_li_list = news_div.find_all('li')

for a in news_li_list:
    news_a = a.find('a')  # 查找li下边每一个li标签包含的a标签
    if news_a:
        # print(news_a.attrs)  # 返回a标签的属性 字典类型 {'href':'//www.autohome.com.cn/news/201803/914177.html#pvareaid=102624'}
        title = news_a.find('h3').text  # 获取新闻的标题
        news_img ='http:%s' % news_a.find('img').attrs.get('src')  # 获取图片链接

        # 发送get请求下载图片
        img_respone = requests.get(url=news_img)
        filename = str(uuid4()) + '.jpg'  # 创建一个文件名
        # 保存文件到本地
        with open(filename, 'wb') as f:
            f.write(img_respone.content)  # 获取的img_respone转换成字节

        print('标题：%s  链接：http:%s  相关图片：%s' % (title, news_a.attrs.get('href'), news_img))  # 获取所有的新闻链接

```
### 抽屉全站自动点赞
```
class Chouti(object):

    def __init__(self, phone, password):
        '''分析：
            1. 抽屉需要先发送一个get请求，获取gpsd
            2.携带第一次get请求获取的gpsd，去登录验证
            3.携带第一次get请求的gpsd去点赞
            :param phone: 抽屉手机号，需要86开头
            :param password: 抽屉登录密码'''

        self._login_post_date = {
            'phone': phone,
            'password': password,
            'oneMonth': 1}
        self._headers = {
            'Origin': 'http://dig.chouti.com',
            'Referer': 'http://dig.chouti.com/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36'
        }

    def start_v2(self):
        session = requests.Session() # 使用Session会自动管理cookie
        session.get(url='http://dig.chouti.com/', headers=self._headers)
        session.post(url='http://dig.chouti.com/login', data=self._login_post_date,headers=self._headers,)
        for index in range(1, 121):
            respones = session.get(url='http://dig.chouti.com/all/hot/recent/%s' % index, headers=self._headers)
            soup = BeautifulSoup(respones.text, 'lxml')
            news_div = soup.find(id='content-list')
            news_list = news_div.find_all('div', attrs={'class': 'item'})
            for i in news_list:
                news_id = i.find('i')
                text = session.post(url='http://dig.chouti.com/link/vote?linksId=%s' % news_id.text, headers=self._headers)
                print(text.text)
```
### 联通手机号查询话费及个人信息
```python
import requests
import time
import re
import json


class GetPhoneTariff(object):

    def __init__(self, phone, password):
        self.session = requests.Session()
        self.phone = phone
        self.password = password
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'}

    def login_unicom(self):
        """
        自动登录中国联通
        :return: boolean， 登录成功返回True，登录失败返回False
        """
        login_headers = self.headers
        login_headers['Host'] = 'uac.10010.com'
        login_headers['Referer'] = 'http://uac.10010.com/portal/hallLogin'

        # 输入密码input框获取焦点的时候发送get请求， 会获取个cookie  ckuuid
        ckuuid_url = 'https://uac.10010.com/portal/Service/CheckNeedVerify?callback=jQuery17206328033706325524_1525102835951&userName=%s&pwdType=01&_=%s' % (
            self.phone, str(int(time.time())))
        self. session.get(url=ckuuid_url, headers=login_headers)
        # 此页面再次获取cookie  uacverifykey
        self.session.get(url='http://uac.10010.com/portal/Service/CreateImage?t=%s' %
                         str(int(time.time())), headers=login_headers)

        # 发送验证码 携带cookie   uacverifykey   ckuuid
        c_time = int(time.time())
        url = 'https://uac.10010.com/portal/Service/SendCkMSG?callback=jQuery17209749269944548633_1525103158591&req_time=%s&mobile=%s&_=%s' % (
            c_time, self.phone, c_time)
        self.session.get(url=url, headers=login_headers)
        # jQuery17209749269944548633_1525103158591({resultCode:"7096",redirectURL:"null",errDesc:"null",msg:'系统忙，请稍后再试。',needvode:"0",errorFrom:"null"});
        # 等待用户输入验证码
        verification = input('请输入验证码:')

        # 发送登录请求， 获取登录cookie
        c_time = int(time.time())
        login_url = 'https://uac.10010.com/portal/Service/MallLogin?callback=jQuery172013683158086553893_1525106413187&req_time=%s&redirectURL=http://www.10010.com&userName=%s&password=%s&pwdType=01&productType=01&redirectType=03&rememberMe=1&verifyCKCode=%s&_=%s' % (
            str(c_time), self.phone, self.password, verification, str(c_time))
        response = self.session.get(url=login_url, headers=login_headers)
        print(response.text)
        code = int(
            re.search(
                r'resultCode:"(?P<code>\d+)"',
                response.text).group('code'))
        if code != 0000:
            msg = re.search(r'msg:\'(?P<msg>.+)\'', response.text).group('msg')
            print('登录失败:%s' % msg)
            return False
        cookie_dict = requests.utils.dict_from_cookiejar(self.session.cookies)
        print(cookie_dict)
        print('登录成功')
        return True

    def init_query_cookie(self):
        """
        初始化 Cookies
        :return:
        """
        self.login_unicom()
        self.headers = {
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8',
            'Host': 'iservice.10010.com',
            'Origin': 'http://iservice.10010.com',
            'Referer': 'http://iservice.10010.com/e4/index_server.html',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36',
            'X-Requested-With': 'XMLHttpRequest'}

        # 处理一个叫e3的cookie
        self.session.post(url='http://iservice.10010.com/e3/static/query/searchPerInfo/?_=%s' %
                          str(int(time.time())), headers=self.headers)
        self.session.post(url='http://iservice.10010.com/e3/static/query/searchPerInfoUser/?_=%s' %
                          str(int(time.time())), headers=self.headers)
        self.session.get(url='http://iservice.10010.com/e3/static/query/message/pageList?_=%s&currentPage=1&pageSize=2' %
                         str(int(time.time())), headers=self.headers)
        self.session.post(url='http://iservice.10010.com/e3/static/check/checklogin/?_=%s' %
                          str(int(time.time())), headers=self.headers)
        self.save_cookies()

    def save_cookies(self):
        """
        保存cookies到数据库
        :return:
        """
        cookies = requests.utils.dict_from_cookiejar(self.session.cookies)

    def get_info(self):
        self.init_query_cookie()
        response = self.session.post(
            url='http://iservice.10010.com/e3/static/query/userinfoquery?_%s' % str(int(time.time())), headers=self.headers)
        return json.loads(response.text)


if __name__ == '__main__':
    a = GetPhoneTariff(phone=手机号, password='服务密码')
    info_dict = a.get_info()
    print(info_dict)
    print('-' * 50)
```
### 抽屉新热榜全站自动点赞-异步并发版
```python
from concurrent.futures import ThreadPoolExecutor, wait
from bs4 import BeautifulSoup
import json
import requests
import time

class Chouti(object):
    '''
    抽屉新热榜全站自动点赞
    '''
    def __init__(self, phone, password):
        # 请求携带的认证信息
        self._login_post_date = {
            'phone': phone,
            'password': password,
            'oneMonth': 1}
        # 请求携带的headers
        self._headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36'
        }
        self.session = requests.Session()
        # 用于存放每条新闻的ID的队列
        # self.links_id = queue.Queue()

    def login_chouti(self):
        '''
        登陆抽屉，主要用于获取cookie
        :return:
        '''
        self.session.get(url='https://dig.chouti.com/', headers=self._headers)
        respones = self.session.post(url='https://dig.chouti.com/login', data=self._login_post_date, headers=self._headers, )  # 请求登录cookie
        # {"result":{"code":"9999", "message":"", "data":{"complateReg":"0","destJid":"cdu_51313511426"}}}
        respones = json.loads(respones.text)
        if respones['result']['code'] == '9999':
            print('登陆抽屉成功')
            return True
        else:
            print('登陆抽屉失败: %s' % respones['result']['message'])
            return False

    def get_chouti_html(self, page):
        '''
        用于获取抽屉的分页
        :param page: 页数
        :return: 响应Rsponse对象
        '''
        respones = self.session.get(url='https://dig.chouti.com/all/hot/recent/%s' % page, headers=self._headers)
        return respones

    def handel_chouti_html(self, future):
        '''
        获取抽屉每个页面中所有新闻的ID
        :param future:
        :return:
        '''
        respones = future.result()
        future.links_id = []
        soup = BeautifulSoup(respones.text, 'lxml')
        news_div = soup.find(id='content-list')
        news_list = news_div.find_all('div', attrs={'class': 'item'})
        for i in news_list:
            news_id = i.find('i')
            future.links_id.append(news_id.text)

    def recommend(self, future):
        '''
        推荐
        :param future:
        :return:
        '''
        for i in future.links_id:
            text = self.session.post(url='https://dig.chouti.com/link/vote?linksId=%s' % i, headers=self._headers)
            print(text.text)

    def cancel_recommend(self, futrre):
        '''
        取消推荐
        :param futrre:
        :return:
        '''
        for i in futrre.links_id:
            text = self.session.post(url='https://dig.chouti.com/vote/cancel/vote.do', data={'linksId': i}, headers=self._headers)
            print(text.text)

    def start(self, option):
        '''
        多线程异步启动
        :param option:  True(推荐) or False(取消推荐)
        :return:
        '''
        self.login_chouti()
        pool = ThreadPoolExecutor(120)
        result = [pool.submit(self.get_chouti_html, i) for i in range(1, 121)]
        for i in result:
            i.add_done_callback(self.handel_chouti_html)
            i.add_done_callback(self.cancel_recommend if not option else self.recommend)

        wait(result)
        # 6分钟
        print('操作完毕')

    def low_start(self):
        '''
        单线程推荐
        :return:
        '''
        self.login_chouti()
        for index in range(1, 121):
            respones = self.session.get(url='https://dig.chouti.com/all/hot/recent/%s' % index, headers=self._headers)
            soup = BeautifulSoup(respones.text, 'lxml')
            news_div = soup.find(id='content-list')
            news_list = news_div.find_all('div', attrs={'class': 'item'})
            for i in news_list:
                news_id = i.find('i')
                text = self.session.post(url='https://dig.chouti.com/link/vote?linksId=%s' % news_id.text,
                                    headers=self._headers)
                print(text.text)


```
### 总结
```
requests:
    respone = requests.get('url') # 发送GET请求
    respone = requests.post(url='url', data={},cookies={}) # 发送POST请求携带参数及cookie
    respone.text # 返回str类型
    respone.content # 返回字节类型
    response.encoding # 指定Response的编码
    response.apparent_encoding # 返回改respones的编码
    response.cookies,get_dict() # 获取cookie的字典形式

    requests.request() 参数详解
        - method：提交方式，post，get，delete， put， head， patch， options
        - url： 提交地址
        - params： 在url中传递参数，GET params = {k1:vi}
        - data: 在请求体里传递参数用于post请求 data = {k1:v1,k2:v2} or 'k1=v1&k2=v2'
        - json: 在请求体里传递参数，并在请求头中设置content-type： application/json
        - headers： 在请求头中添加数据
        - cookies: 网站cookies 在请求头中
        - files : 文件对象{'f1': open('s1.py', wb), 'f2': ('上传到服务器的文件名', oprn('s1.py', wb))}
        - auth : 认证使用 在 请求头中加入用户名密码
        - timeout ： 超时时间
        - allow_redirects: 是否允许重定向 bool
        - proxies: 代理
        = stream: 流,bool   用于下载文件
            ret = request.get('http://127.0.0.1:8888/test/', steam=True)
            for i in ret.iter.content():
                print(i)
         - cert: 证书 指定https SSL证书文件
         - verify = False https忽略证书存在

     ret = request.Session() 用来保存客户端历史访问信息
        - ret.get(),ret.post()  # 会自动携带cookie信息









beautifuilsoup4: 用于解析html标签
    soup = BeautifulSoup(response.text, 'lxml')

    tag = soup.find(id='li') # 查找第一个id为li的标签
    tag = soup.find(name='div' id='item', attrs={'class': 'list'}, recursive=True, text='Lacie'， class_='item)
        - name： 标签名
        - id：标签的id属性
        - attrs：标签的属性
        - recursive：是否递归的去查找，false 只在儿子里找， true 在子子孙孙也找
        - text：标签的文本信息
        - class_:标签的class属性

    tag_list = soup.find_all('div) # fing_all 查找所有div 返回一个list 参数名同find

    # select 同css的选择器
    tag = soup.select('#li')

    tag.text # 返回标签的文本
    tag.string # 返回标签的内容
    tag.stripped_string # 递归循环获取内部所有标签文本
    tag.name # 返回标签名称
    tag.attrs # 返回标签的属性 字典类型

    tag.attre = {'id': 'hello'} # 重新赋值

    tag.clear() # 清空内部所有标签
    tag.decompose() # 清空所有标签包含自己
    tag.extract() # 同decompose 但是会返回删除的标签

    tag.encode() # 返回标签的字节类型
    tag.decode() # 返回标签的字符串类型
    tag.encode_contents() # 返回标签的内部内容字节类型
    tag.decode_contents() # 返回标签的内部内容字符串类型

    tag.get('id') # 获取标签的id属性
    tag.has_attr('id') # 判断当前标签是否有id这个属性
    tag.get_text() # 获取当前标签的文本

    tag.index(tag,find('div')) # 查找标签在某个标签中的索引位置
    tag.is_empty_element() 判断是否是自闭和标签： 如果 br,hr,input,img,meta,soacer,link,frame,base

    # 当前标签的关联标签，同jquery的链式调用
    tag.next
    tag.next_element
    tag.next_elements
    tag.next_sibling
    tag.next_siblings
    tag.previous
    tag.previous_element
    tag.previous_elements
    tag.previous_sibling
    tag.previous_siblings
    tag.parend
    tag.parents

    tag.children # 当前标签的所有儿子标签 返回一个列表
        children会返回换行
        # 获取所有标签，不要文本
        from bs4.element import Tag:
        for tag in tag.children:
            if type(tag) == Tag:
                print(tag)

    tag.descendants # 当前元素的所有子子孙孙标签

    # 当前标签的关联标签并支持筛选的 参数同find_all
    tag.fint_next()
    tag.fint_next_element()
    tag.fint_next_elements()
    tag.fint_next_sibling()
    tag.fint_next_siblings()
    tag.fint_previous()
    tag.fint_previous_element()
    tag.fint_previous_elements()
    tag.fint_previous_sibling()
    tag.fint_previous_siblings()
    tag.fint_parend()
    tag.fint_parents()


    from bs4.element import Tag:
    a = Tag(name='a', attrs={'herf':'http://www.baidu.com/'})
    tag = soup.find('body)

        tag..append(a)# 创建一个元素用于追加到当前元素后边
        tag.insert(2,a) # 在当前元素中插入一个元素
        tag.insert_after(a) # 在当前标签前插入
        tag.insert_before(a) # 在当前标签后插入
        tag.replace_with() # 当前标签替换为指定标签
        tag.wrap(a) # 包裹某个表情
        tag.unwrap() # 去除当前标签的包裹
```



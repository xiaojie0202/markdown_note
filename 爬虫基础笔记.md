# 网络爬虫
    顾名思义爬取网络资源的程序或脚本， 爬虫会安装一定的规则取爬去需要的信息
## 爬虫模块 request
    Requests 是使用 Apache2 Licensed 许可证的 基于Python开发的HTTP 库，其在Python内置模块的基础上进行了高度的封装，从而使得Pythoner进行网络请求时，变得美好了许多，使用Requests可以轻而易举的完成浏览器可有的任何操作。
### 安装
    pip3 install requests
### request模块详解
>- request 请求
```
respone = requests.get('url')  # 发送GET请求
respone = requests.post(url='url', data={},cookies={})  # 发送POST请求
respone = requests.put(url, data=None, **kwargs)
respone = requests.head(url, **kwargs)
respone = requests.delete(url, **kwargs)
respone = requests.patch(url, data=None, **kwargs)
respone = requests.options(url, **kwargs)

# 以上requests的方法都是基于requests.request()
 requests.request()
        - method：提交方式，post，get，delete， put， head， patch， options
        - url： 提交地址
        - params： 在url中传递参数，GET params = {k1:vi}
        - data: 在请求体里传递参数用于post请求 data = {k1:v1,k2:v2} or 'k1=v1&k2=v2'
        - json: 在请求体里传递参数，并在请求头中设置content-type： application/json
        - headers： 在请求头中添加数据
        - cookies: 网站cookies 在请求头中
        - files : 文件对象{'f1': open('s1.py', wb), 'f2': ('上传到服务器的文件名', oprn('s1.py', wb))}
        - auth : 认证使用 在 请求头中加入用户名密码
        - timeout ： 超时时间
        - allow_redirects: 是否允许重定向 bool
        - proxies: 代理
        = stream: 流,bool   用于下载文件
            ret = request.get('http://127.0.0.1:8888/test/', steam=True)
            for i in ret.iter.content():
                print(i)
         - cert: 证书 指定https SSL证书文件
         - verify = False https忽略证书存在

```
>- 获取请求结果
```
respone.text # 返回str类型
respone.content # 返回字节类型
response.encoding # 指定Response的编码
response.apparent_encoding # 返回改respones的编码
response.cookies,get_dict() # 获取cookie的字典形式
```
>- requests模块高阶函数 request.Session
```
request.Session: 自动管理Cookies信息
ret = request.Session()
ret.get('https://www.baidu.com')

```

## 解析response内容
    当爬虫爬去数据的时候，使用  bs4 来解析爬去的内容
## 安装
    pip3 install beautifuilsoup4
## bs4 模块详解
```
# 导入模块 fron bs4 import beautifuilsoup4

beautifuilsoup4: 用于解析html标签
    soup = BeautifulSoup(response.text, 'lxml')

    tag = soup.find(id='li') # 查找第一个id为li的标签
    tag = soup.find(name='div' id='item', attrs={'class': 'list'}, recursive=True, text='Lacie'， class_='item)
        - name： 标签名
        - id：标签的id属性
        - attrs：标签的属性
        - recursive：是否递归的去查找，false 只在儿子里找， true 在子子孙孙也找
        - text：标签的文本信息
        - class_:标签的class属性

    tag_list = soup.find_all('div) # fing_all 查找所有div 返回一个list 参数名同find

    # select 同css的选择器
    tag = soup.select('#li')

    tag.text # 返回标签的文本
    tag.string # 返回标签的内容
    tag.stripped_string # 递归循环获取内部所有标签文本
    tag.name # 返回标签名称
    tag.attrs # 返回标签的属性 字典类型

    tag.attre = {'id': 'hello'} # 重新赋值

    tag.clear() # 清空内部所有标签
    tag.decompose() # 清空所有标签包含自己
    tag.extract() # 同decompose 但是会返回删除的标签

    tag.encode() # 返回标签的字节类型
    tag.decode() # 返回标签的字符串类型
    tag.encode_contents() # 返回标签的内部内容字节类型
    tag.decode_contents() # 返回标签的内部内容字符串类型

    tag.get('id') # 获取标签的id属性
    tag.has_attr('id') # 判断当前标签是否有id这个属性
    tag.get_text() # 获取当前标签的文本

    tag.index(tag,find('div')) # 查找标签在某个标签中的索引位置
    tag.is_empty_element() 判断是否是自闭和标签： 如果 br,hr,input,img,meta,soacer,link,frame,base

    # 当前标签的关联标签，同jquery的链式调用
    tag.next
    tag.next_element
    tag.next_elements
    tag.next_sibling
    tag.next_siblings
    tag.previous
    tag.previous_element
    tag.previous_elements
    tag.previous_sibling
    tag.previous_siblings
    tag.parend
    tag.parents

    tag.children # 当前标签的所有儿子标签 返回一个列表
        children会返回换行
        # 获取所有标签，不要文本
        from bs4.element import Tag:
        for tag in tag.children:
            if type(tag) == Tag:
                print(tag)

    tag.descendants # 当前元素的所有子子孙孙标签

    # 当前标签的关联标签并支持筛选的 参数同find_all
    tag.fint_next()
    tag.fint_next_element()
    tag.fint_next_elements()
    tag.fint_next_sibling()
    tag.fint_next_siblings()
    tag.fint_previous()
    tag.fint_previous_element()
    tag.fint_previous_elements()
    tag.fint_previous_sibling()
    tag.fint_previous_siblings()
    tag.fint_parend()
    tag.fint_parents()


    from bs4.element import Tag:
    a = Tag(name='a', attrs={'herf':'http://www.baidu.com/'})
    tag = soup.find('body)

        tag..append(a)# 创建一个元素用于追加到当前元素后边
        tag.insert(2,a) # 在当前元素中插入一个元素
        tag.insert_after(a) # 在当前标签前插入
        tag.insert_before(a) # 在当前标签后插入
        tag.replace_with() # 当前标签替换为指定标签
        tag.wrap(a) # 包裹某个表情
        tag.unwrap() # 去除当前标签的包裹
```

## 实列
### 爬取汽车之家新闻，并下载新闻图片到本地
```
import requests
from bs4 import BeautifulSoup
from uuid import uuid4

# 发送get请求下载整个网站
response = requests.get(
    url='https://www.autohome.com.cn/news/')

# 指定编码为此网站的编码
response.encoding = response.apparent_encoding

# response.text 所有HTML标签的文本格式

# 下载的html文本转换成对象， 第二个参数指定html解析器为 lxml
soup = BeautifulSoup(response.text, 'lxml')

# 获取包含所有新闻列表的DIV
news_div = soup.find(id='auto-channel-lazyload-article')

# 查找news_div下的所有li标签  list类型
news_li_list = news_div.find_all('li')

for a in news_li_list:
    news_a = a.find('a')  # 查找li下边每一个li标签包含的a标签
    if news_a:
        # print(news_a.attrs)  # 返回a标签的属性 字典类型 {'href':'//www.autohome.com.cn/news/201803/914177.html#pvareaid=102624'}
        title = news_a.find('h3').text  # 获取新闻的标题
        news_img ='http:%s' % news_a.find('img').attrs.get('src')  # 获取图片链接

        # 发送get请求下载图片
        img_respone = requests.get(url=news_img)
        filename = str(uuid4()) + '.jpg'  # 创建一个文件名
        # 保存文件到本地
        with open(filename, 'wb') as f:
            f.write(img_respone.content)  # 获取的img_respone转换成字节

        print('标题：%s  链接：http:%s  相关图片：%s' % (title, news_a.attrs.get('href'), news_img))  # 获取所有的新闻链接

```
### 抽屉全站自动点赞
```
class Chouti(object):

    def __init__(self, phone, password):
        '''分析：
            1. 抽屉需要先发送一个get请求，获取gpsd
            2.携带第一次get请求获取的gpsd，去登录验证
            3.携带第一次get请求的gpsd去点赞
            :param phone: 抽屉手机号，需要86开头
            :param password: 抽屉登录密码'''

        self._login_post_date = {
            'phone': phone,
            'password': password,
            'oneMonth': 1}
        self._headers = {
            'Origin': 'http://dig.chouti.com',
            'Referer': 'http://dig.chouti.com/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36'
        }

    def start_v2(self):
        session = requests.Session() # 使用Session会自动管理cookie
        session.get(url='http://dig.chouti.com/', headers=self._headers)
        session.post(url='http://dig.chouti.com/login', data=self._login_post_date,headers=self._headers,)
        for index in range(1, 121):
            respones = session.get(url='http://dig.chouti.com/all/hot/recent/%s' % index, headers=self._headers)
            soup = BeautifulSoup(respones.text, 'lxml')
            news_div = soup.find(id='content-list')
            news_list = news_div.find_all('div', attrs={'class': 'item'})
            for i in news_list:
                news_id = i.find('i')
                text = session.post(url='http://dig.chouti.com/link/vote?linksId=%s' % news_id.text, headers=self._headers)
                print(text.text)
```
### 总结
```
requests:
    respone = requests.get('url') # 发送GET请求
    respone = requests.post(url='url', data={},cookies={}) # 发送POST请求携带参数及cookie
    respone.text # 返回str类型
    respone.content # 返回字节类型
    response.encoding # 指定Response的编码
    response.apparent_encoding # 返回改respones的编码
    response.cookies,get_dict() # 获取cookie的字典形式

    requests.request() 参数详解
        - method：提交方式，post，get，delete， put， head， patch， options
        - url： 提交地址
        - params： 在url中传递参数，GET params = {k1:vi}
        - data: 在请求体里传递参数用于post请求 data = {k1:v1,k2:v2} or 'k1=v1&k2=v2'
        - json: 在请求体里传递参数，并在请求头中设置content-type： application/json
        - headers： 在请求头中添加数据
        - cookies: 网站cookies 在请求头中
        - files : 文件对象{'f1': open('s1.py', wb), 'f2': ('上传到服务器的文件名', oprn('s1.py', wb))}
        - auth : 认证使用 在 请求头中加入用户名密码
        - timeout ： 超时时间
        - allow_redirects: 是否允许重定向 bool
        - proxies: 代理
        = stream: 流,bool   用于下载文件
            ret = request.get('http://127.0.0.1:8888/test/', steam=True)
            for i in ret.iter.content():
                print(i)
         - cert: 证书 指定https SSL证书文件
         - verify = False https忽略证书存在

     ret = request.Session() 用来保存客户端历史访问信息
        - ret.get(),ret.post()  # 会自动携带cookie信息









beautifuilsoup4: 用于解析html标签
    soup = BeautifulSoup(response.text, 'lxml')

    tag = soup.find(id='li') # 查找第一个id为li的标签
    tag = soup.find(name='div' id='item', attrs={'class': 'list'}, recursive=True, text='Lacie'， class_='item)
        - name： 标签名
        - id：标签的id属性
        - attrs：标签的属性
        - recursive：是否递归的去查找，false 只在儿子里找， true 在子子孙孙也找
        - text：标签的文本信息
        - class_:标签的class属性

    tag_list = soup.find_all('div) # fing_all 查找所有div 返回一个list 参数名同find

    # select 同css的选择器
    tag = soup.select('#li')

    tag.text # 返回标签的文本
    tag.string # 返回标签的内容
    tag.stripped_string # 递归循环获取内部所有标签文本
    tag.name # 返回标签名称
    tag.attrs # 返回标签的属性 字典类型

    tag.attre = {'id': 'hello'} # 重新赋值

    tag.clear() # 清空内部所有标签
    tag.decompose() # 清空所有标签包含自己
    tag.extract() # 同decompose 但是会返回删除的标签

    tag.encode() # 返回标签的字节类型
    tag.decode() # 返回标签的字符串类型
    tag.encode_contents() # 返回标签的内部内容字节类型
    tag.decode_contents() # 返回标签的内部内容字符串类型

    tag.get('id') # 获取标签的id属性
    tag.has_attr('id') # 判断当前标签是否有id这个属性
    tag.get_text() # 获取当前标签的文本

    tag.index(tag,find('div')) # 查找标签在某个标签中的索引位置
    tag.is_empty_element() 判断是否是自闭和标签： 如果 br,hr,input,img,meta,soacer,link,frame,base

    # 当前标签的关联标签，同jquery的链式调用
    tag.next
    tag.next_element
    tag.next_elements
    tag.next_sibling
    tag.next_siblings
    tag.previous
    tag.previous_element
    tag.previous_elements
    tag.previous_sibling
    tag.previous_siblings
    tag.parend
    tag.parents

    tag.children # 当前标签的所有儿子标签 返回一个列表
        children会返回换行
        # 获取所有标签，不要文本
        from bs4.element import Tag:
        for tag in tag.children:
            if type(tag) == Tag:
                print(tag)

    tag.descendants # 当前元素的所有子子孙孙标签

    # 当前标签的关联标签并支持筛选的 参数同find_all
    tag.fint_next()
    tag.fint_next_element()
    tag.fint_next_elements()
    tag.fint_next_sibling()
    tag.fint_next_siblings()
    tag.fint_previous()
    tag.fint_previous_element()
    tag.fint_previous_elements()
    tag.fint_previous_sibling()
    tag.fint_previous_siblings()
    tag.fint_parend()
    tag.fint_parents()


    from bs4.element import Tag:
    a = Tag(name='a', attrs={'herf':'http://www.baidu.com/'})
    tag = soup.find('body)

        tag..append(a)# 创建一个元素用于追加到当前元素后边
        tag.insert(2,a) # 在当前元素中插入一个元素
        tag.insert_after(a) # 在当前标签前插入
        tag.insert_before(a) # 在当前标签后插入
        tag.replace_with() # 当前标签替换为指定标签
        tag.wrap(a) # 包裹某个表情
        tag.unwrap() # 去除当前标签的包裹
```


